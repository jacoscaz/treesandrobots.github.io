---
title: "Am I a large language model?"
description: ""
---

_Salasso_ is the italian word for [bloodletting][1], the withdrawal of blood
from a patient to prevent or cure illness and disease. Although abandoned by
virtually all of modern medicine as being overlwemingly harmful to patients,
the word _salasso_ is still used as a metaphor for very signifcant expenses,
draining a person of their funds as leeches would have drained them of their
blood.

The use of _salasso_ as a metaphor is not common but, for some reason, it has
always been a part of my family's vocabulary; I do not remember ever learning
the word and I assume I must have picked it up and started using it at a very
young age. However, even though I was aware of the long-gone medical practice
of bloodletting, I only found out we had a word for it throughout my twenties
(and not without a certain amount of embarassment). Up to then, I had always
understood _salasso_ as being just another noun for a significant expense and
never thought much of it.

In principle, the way I picked up and started using the word _salasso_ seems
to me to be entirely similar to the process that allows today's large language
models to approximate our use of language to such an incredible degree. As a
child I was never aware of the word's literal meaning, possessing no facts
about the word itself. As a good pattern recognition machine, however, I must
have picked up on the fact that _salasso_ would repeatedly appear in contexts
that implied the presence of a significant expense and in which _salasso_ was
the only word that could carry that very meaning.

Models such as the GPTs (ChatGPT, GPT-4, ...) do not _know_ anything about the
words that they use but they're still able to concatenate them following the
rules of language. In a way, they appear to establish connections betweek words
not unlike how I was able to connect _salasso_ to its metaphorical meaning...
For the entirety of human language. I'd be remiss if I didn't cite at least a
couple of introductory articles to the world of LLM: Stephen Wolfram's 
[What is ChatGPT doing and why does it work?][3] and Confused bit's
[How does GPT work?][2].

Our common vocabulary is ill-equipped to describe and talk about large
language models. Words like _know_, _think_, _infer_, _hallucinate_ are
generally understood to imply a level of conscious thought and understanding
that, so far, appear to remain absent from these models.

On one side, the use of these words casts these models into a light that is
entirely too human, at least for the time being. Most of us agree that what
we are seeing does not classify as _thinking_ in the same way that we think
and that a model cannot _be wrong_ in the same way that we can. What should
we call it, then? How can we ever expect our children and elderly to
understand the difference between ourselves and these models, at least insofar
as a difference continues to exist, unless we develop the appropriate
vocabulary for it?

I think a good starting point might be the concept of _prediction_, making the
comparison with atmospheric models used for weather forecasts. Both of these 
models make predictions, albeit in two different domains: the domain of climate
sciences and the domain of language. Still, nobody believes that an atmospheric
model may ever give rise to _thinking_, even as it runs on the very best of
today's supercomputers. Why should we treat a large language model differently?
The ability of these models to make accurate predictions is not indicative of
their potential for thinking and understanding.

On the other side, however, I am finding it more and more difficult to argue
against the fact that I, too, am very likely to be a large language model of
my own, deployed in a bipedal, biological machine that can form and persist
memories, in a community of similarly equipped machines. A model that might
have emerged by sheer chance but then quickly proved to be the greatest 
evolutionary advantage that life had ever manifested.

Perhaps the question should not be whether I am one such model or not but
whether there's anything more to me that goes beyond a large language model
and what is it.

[1]: https://en.wikipedia.org/wiki/Bloodletting
[2]: https://confusedbit.dev/posts/how_does_gpt_work/
[3]: https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
